---
title: "R notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Lessons learned

  - Don't mix **tidyverse**, data frame and **data.table** concepts in processing; do one then the other, else data.table attributes get stripped off
  - Don't try to remove profanity word by word: iterating over 1000 profane items x 4 million lines even using stri_remove_all_fixed takes a **lot** of time
  - Different R packages have different design philosophies, some are more suited to smaller data processing tasks
  
# Selections
test <- sqldf("select * from t1 where t1.n > 400 limit 1")  
test <- sqldf("select * from t1 where t1.ngram = 'i' ")
test <- sqldf("select * from t2 where t2.ngram like 'i%' ")
    The percent sign % wildcard matches any sequence of zero or more characters. http://www.sqlitetutorial.net/sqlite-like/
    The underscore _ wildcard matches any single character.
test <- sqldf("select * from t2 where t2.ngram like 'i %' ")  #find i, space, wildcards.
> head(test)
    ngram    n
1  i love 1499
2  i have 1297

test <- t2[ngram=="i am"]
#Sizes of objects
  sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))
  format(object.size(corpus), units="auto")
  To get memory usage for your namespace, by object type, use memory.profile()
#Reversing ngrams 
  See handling strings in r by sanchez
  
#Timing R
  t1 <- Sys.time()
  print(paste0("string: ", difftime(Sys.time(), t1, units = 'sec')))
  system.time(expression)
  
#Length of a vector vs nrow/ncol
 lengths(ngram1)
   ngram      n 
  904589 904589 
  nrow/ncol(object)
  
#Regex 
? optional previous character: colou?r matches color, colour
oo*h 0 or more of prev char
00+  1 or more of prev char
. single char

#Converting to character vectors
* convert the column to a character not the entire table eg profdf$profanity, not profdf
str(profdf)
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	3099 obs. of  1 variable:
 $ profanity: chr  "damn" "dyke" "fuck" "shit" ...

p <- as.character(profdf$profanity) ; 
[1] "damn"          "dyke"          "fuck"          "shit"          "2 girls 1 cup" "2g1c"         
p2 <- as.character(profdf); head(p2)
[1] "c(\"damn\", \"dyke\", \"fuck\", \"shit\", \"2 girls 1 cup\", \"2g1c\", \"4r5e\", \"5h1t\",etc

# Comparing sqldf
test <- sqldf("select * from t1 where t1.n > 400 limit 1")  
test <- sqldf("select * from t1 where t1.ngram = 'i' ")
test <- sqldf("select * from t2 where t2.ngram like 'i%' ")
    The percent sign % wildcard matches any sequence of zero or more characters. http://www.sqlitetutorial.net/sqlite-like/
    The underscore _ wildcard matches any single character.
test <- sqldf("select * from t2 where t2.ngram like 'i %' ")  #find i, space, wildcards.
> head(test)
    ngram    n
1  i love 1499
2  i have 1297

test <- t2[ngram=="i am"]
#get current data sizes
  sort(sapply(ls(), function(x) format(object.size(get(x)), unit = 'auto')))
  format(object.size(corpus), units="auto")

# Reversing ngrams handling strings in r by sanchez
# Timing
  t1 <- Sys.time()
  print(paste0("string: ", difftime(Sys.time(), t1, units = 'sec')))
  system.time(expression)
  
# Length

 lengths(ngram1)
  ngram      n 
  904589 904589 
  nrow/ncol(object)
  use nchar for the length of elements inside a character vector
  
#Regex
? optional previous character: colou?r matches color, colour
oo*h 0 or more of prev char
00+  1 or more of prev char
. single char


system.time({
 sqldf("select ngram from ngram3 where ngram like 'Cat is %' limit 10")
})
# Using sqldf lookup vs datatable like() lookup
> pred("cat is")
[1] "p2_1g: is a: a"
[1] "p3_2g: cat is in: in"
[1] "p4_3g: : NoNgram"
[1] "p5_4g: : NoNgram"
[1] "SQL lookup time: 15.4968512058258"
[1] "p2_1g: is a: a"
[1] "p3_2g: cat is in: in"
[1] "p4_3g: NA: NA"
[1] "p5_4g: NA: NA"
[1] "DataTable lookup time: 3.44549703598022"


> unkwords <- ngram1[n==1, c("ngram") ]
>   unkwords <- data.frame(unkwords[ , replacement:="unk"]) #DataCombine
>   corpusold <- data.frame(corpus)
> system.time({
+   unkcorpus <- DataCombine::FindReplace( data=corpusold, Var="text", replaceData=unkwords,
+                             from="ngram", to="replacement", exact=FALSE)
+   })
    user   system  elapsed 
44982.18    29.08 45210.42 

> system.time({
+   unkcorpusstringi <- stri_replace_all_fixed(str=corpus$text, pattern=unkwords$ngram, replacement=unkwords$replacement,vectorize_all=FALSE )
+   })
   user  system elapsed 
6994.91    0.04 7012.03 

# data.table display in Rstudio environment pane
A data.table object in the environment panel will not update its preview after new variables are added using the := method. However str(dt) shows the correct details, and assigning dt to a new variable results in the correct preview in the Environment panel. ??maybe because update by reference?? Rstudio doesn't catch this.

# data.table adding multiple columns
Adding multiple columns at once with DT
NEWCOLS <- c("w1", "w2", "w3", "w4", "w5")

x2[, c("w1", "w2") := NULL ]         #remove columns. x3 <- x2 by reference NOT copy.
x3[ , w1 := str_split(ngram, " ")[[1]][1] ]  #add cols w1 w2 works ok, don't need WITH
x3[ , w2 := str_split(ngram, " ")[[1]][2] ] 
# OK x3[ , c("w1", "w2") := list(ngram, nchar(ngram)) ]  #add cols w1 w2 works ok, don't need WITH

# Object sizes after removing ngram strings
> object.size(ngram2)
230714712 bytes
> ngram2[, ngram := NULL]
> object.size(ngram2)
92735216 bytes
> str(ngram2)

# Shiny
Errors:
object of type 'closure' is not subsettable: usually means you've referenced an object as name rather than name()

# Corpus ngram sizes n, pr, lpr, individual words
#full corpus
object.size(ngram1)
14300344 bytes
object.size(ngram2)
101058576 bytes
 66584376 after
object.size(ngram3)
236161984 bytes
152305344 bytes after
object.size(ngram4)
327658688 bytes
194668424 bytes after
object.size(ngram5) before
405145640 bytes
> object.size(ngram5) after
231798560 bytes

  #object.size(ngram1)
  #14300344 bytes
  #object.size(ngram1) after adding index
  #14973544 bytes     #673200 bytes larger
  # Replace all words in all grams with an integer
  #Get a list of w* column names from ng, except w1, as that's our index.
  #  #This will subset the DT and return just the cols: ngram3[, grep("w[2-5]?", names(ngram3)), with=FALSE]
  
  # Data.table indexes
   setkey(ngram2, w1, w2, i1, i2)
   can't then ngram2[.(int, int)] #...because the first two indexes are characters.
   
     #Quiz 1 ----
    # fsize = file.size(blogFile)
    # print(paste0("blogs file size: ",fsize))
    # 
    # #1.2
    # zTwitLen <- length(twitData)
    # print(paste0("en_US.twitter.txt has ",zTwitLen, " lines of text" ))
    # 
    # #1.3
    # maxLLtwit <- max(str_length(twitData))
    # maxLLblog <- max(str_length(blogData))
    # maxLLnews <- max(str_length(newsData))
    # print(paste0("Max twitter line length: ", maxLLtwit))
    # print(paste0("Max blog    line length: ", maxLLblog))
    # print(paste0("Max news    line length: ", maxLLnews))
    # 
    # #1.4
    # nlineslove <- sum(str_count(twitData, "love")) 
    # nlineshate <- sum(str_count(twitData, "hate")) 
    # print(paste0("Love to hate ratio: ", nlineslove/nlineshate))
    # 
    # #1.5
    # #one tweet mentions "biostats". what is the full tweet?
    # str_subset(twitData,"biostats")
    # #[1] "i know how you feel.. i have biostats on tuesday and i have yet to study =/"
    # 
    # # 1.6
    # # find how many tweets contain "A computer once beat me at chess, but it was no match for me at kickboxing"
    # sum(str_count(twitData, "A computer once beat me at chess, but it was no match for me at kickboxing"))
